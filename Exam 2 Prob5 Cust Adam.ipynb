{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a141622e-ec78-4f29-9541-451b951fc961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Custom Adam Optimizer...\n",
      "Epoch: 010, Loss: 0.6884, Val Acc: 0.5000, Test Acc: 0.5000\n",
      "Epoch: 020, Loss: 0.6533, Val Acc: 0.5085, Test Acc: 0.5066\n",
      "Epoch: 030, Loss: 0.5663, Val Acc: 0.6803, Test Acc: 0.6945\n",
      "Epoch: 040, Loss: 0.5253, Val Acc: 0.6935, Test Acc: 0.7021\n",
      "Epoch: 050, Loss: 0.5106, Val Acc: 0.6850, Test Acc: 0.7059\n",
      "Epoch: 060, Loss: 0.4822, Val Acc: 0.7021, Test Acc: 0.7249\n",
      "Epoch: 070, Loss: 0.4747, Val Acc: 0.7173, Test Acc: 0.7324\n",
      "Epoch: 080, Loss: 0.4644, Val Acc: 0.7230, Test Acc: 0.7324\n",
      "Epoch: 090, Loss: 0.4626, Val Acc: 0.7287, Test Acc: 0.7495\n",
      "Epoch: 100, Loss: 0.4551, Val Acc: 0.7343, Test Acc: 0.7505\n",
      "---\n",
      "Final Link Prediction Classification Accuracy: 0.7505\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.optim import Optimizer\n",
    "import math\n",
    "\n",
    "# DEFINING THE CUSTOM ADAM OPTIMIZER\n",
    "class CustomAdam(Optimizer):\n",
    "    \"\"\"\n",
    "    A custom implementation of the Adam Optimizer.\n",
    "    Formula:\n",
    "    m_t = beta1 * m_{t-1} + (1 - beta1) * g_t\n",
    "    v_t = beta2 * v_{t-1} + (1 - beta2) * g_t^2\n",
    "    theta = theta - lr * m_t / (sqrt(v_t) + epsilon)\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.01, betas=(0.9, 0.999), eps=1e-8):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps)\n",
    "        super(CustomAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values (Momentum)\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values (Variance)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # Update Momentum (First Moment)\n",
    "                # m_t = beta1 * m_{t-1} + (1 - beta1) * g_t\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                # Update Variance (Second Moment)\n",
    "                # v_t = beta2 * v_{t-1} + (1 - beta2) * (g_t)^2\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                # Bias Correction \n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                \n",
    "                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
    "                step_size = group['lr'] / bias_correction1\n",
    "\n",
    "                # Update Weights\n",
    "                # p = p - step_size * (m_t / denom)\n",
    "                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "        return loss\n",
    "\n",
    "# DATASET SETUP\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora', transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "# Link Prediction Split\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    is_undirected=True,\n",
    "    add_negative_train_samples=False\n",
    ")\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "# MODEL ARCHITECTURE\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # Increased hidden channels slightly for better capacity\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        # Added dropout to prevent overfitting now that the optimizer is stronger\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "model = Net(in_channels=dataset.num_features, hidden_channels=128, out_channels=64)\n",
    "\n",
    "# USING THE CUSTOM ADAM OPTIMIZER\n",
    "# Set LR, learning rate to 0.01 (Adam handles the decay internally)\n",
    "optimizer = CustomAdam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# TRAINING LOOP\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # Negative Sampling\n",
    "    pos_edge_index = train_data.edge_label_index\n",
    "    neg_edge_index = torch.randint(0, train_data.num_nodes, pos_edge_index.size(), dtype=torch.long)\n",
    "    \n",
    "    edge_label_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1)\n",
    "    # Labels: 1 for real, 0 for fake\n",
    "    edge_label = torch.cat([torch.ones(pos_edge_index.size(1)), \n",
    "                            torch.zeros(neg_edge_index.size(1))], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).sigmoid()\n",
    "    preds = (out > 0.5).float()\n",
    "    correct = (preds == data.edge_label).sum().item()\n",
    "    return correct / data.edge_label.size(0)\n",
    "\n",
    "# EXECUTION\n",
    "print(\"Training with Custom Adam Optimizer...\")\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_acc = test(val_data)\n",
    "    test_acc = test(test_data)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "print(\"---\")\n",
    "print(f\"Final Link Prediction Classification Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f264f9-dab0-4cb2-994b-fb5ca27af5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
